{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Introduction \n",
    "============\n",
    "\n",
    "The purpose of this document is to expose in a comprehensive way how the\n",
    "spike sorting of a \"large\" data set can be performed with \"simple\" tools\n",
    "built around the `Python` language. The data were recorded from a locust\n",
    "*Schistocerca americana* antennal lobe (the first olfactory relay,\n",
    "equivalent of the *olfactory bulb* of vertebrates). A total of 1 hour\n",
    "and 40 minutes of spontaneous activity was recorded as well as responses\n",
    "to 150 stimulation with citral. The set is publicly available on\n",
    "[zenodo](https://zenodo.org/record/21589) (DOI:\n",
    "[10.5281/zenodo.21589](http://dx.doi.org/10.5281/zenodo.21589)). The\n",
    "recording setting is described in Pouzat, Mazor and Laurent (2002)---[Using noise signature to optimize spike-sorting and to assess neuronal classification quality.](http://xtof.perso.math.cnrs.fr/pdf/Pouzat+:2002.pdf) *Journal of Neuroscience Methods* **122**: 43-57---and a picture of the recording situation can be seen of the third slide\n",
    "of Pouzat (2014)---Peri-Stimulus Time Histograms Estimation Through Poisson Regression Without Generalized Linear Models. Zenodo. [10.5281/zenodo.14660](http://dx.doi.org/10.5281/zenodo.14660). The purpose of these long recordings was probing\n",
    "interactions between neurons and how they are modified by a stimulus.\n",
    "\n",
    "There is no claim that the analysis presented in the sequel is \"The\" way\n",
    "to analyze these data; it is just one *working* way. The motivation, as\n",
    "a referee, is to have an explicit example to show to authors who all too\n",
    "often tend to analyze their data *en bloc*. I'm advocating instead a\n",
    "piecemeal approach were a first stretch of data is initially used to\n",
    "build a model--that is, a catalog of waveform, one per neuron and per\n",
    "recording site--while template matching is applied to the subsequent\n",
    "recorded minutes using a simple trend tracking.\n",
    "\n",
    "The following analysis was performed with the\n",
    "[anaconda](https://store.continuum.io/cshop/anaconda/) distribution of\n",
    "`Python 3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Getting the data\n",
    "================\n",
    "\n",
    "The data are stored in [HDF5](http://www.hdfgroup.org/HDF5/) format on\n",
    "the [zenodo](https://zenodo.org/) server. They are all contained in a\n",
    "file named `locust20010201.hdf5`. The data within this file have an\n",
    "hierarchical organization similar to the one of a file system (one of\n",
    "the main ideas of the HDF5 format).\n",
    "\n",
    "The data can be downloaded and loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "name = 'locust20010201.hdf5'\n",
    "distantfile = 'https://zenodo.org/record/21589/files/'+name\n",
    "localfile = name\n",
    "if not os.path.exists(localfile):\n",
    "    urlretrieve(distantfile, localfile)\n",
    "hdf = h5py.File(localfile,'r')\n",
    "\n",
    "# read 3 trials (=3 segments)\n",
    "ch_names = ['ch09','ch11','ch13','ch16']\n",
    "trial_names = ['trial_01', 'trial_02', 'trial_03']\n",
    "sigs_by_trials = []\n",
    "for trial_name in trial_names:\n",
    "    sigs = np.array([hdf['Continuous_1'][trial_name][name][...] for name in ch_names]).transpose()\n",
    "    sigs = (sigs.astype('float32') - 2**15.) / 2**15\n",
    "    sigs_by_trials.append(sigs)\n",
    "\n",
    "sampling_rate = 15000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Exploring `HDF5` files\n",
    "\n",
    "The `h5py` module provides an extensive access to `HDF5` files many features like the [attributes](http://www.hdfgroup.org/HDF5/doc/UG/HDF5_Users_Guide-Responsive%20HTML5/index.html#t=HDF5_Users_Guide/Attributes/HDF5_Attributes.htm). The file we just opened has for instance a _LabBook_ attribute that we can visualize with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal: young adult female\n",
      "The data come from the second probe penetration in the right antennal lobe.\n",
      "Nice activity on tetrode 9/11/13/16 with response to citral.\n",
      "\n",
      "Continuous_1: 90 acquisitions 29 seconds long with 1 s between end and start.\n",
      "Continuous_2: 20 acquisitions 29 seconds long with 1 s between end and start. 30 MICROMETERS DEEPER TO RECOVER STRONG SIGNAL.\n",
      "Citral_1: 50 stimulations with pure citral (3 s before / 1 s citral / 25 s after) 1 s between end and start. AT THE END FEW DROPS OF SOLUTION AND PROBE MOVED 10 MICROMETERS DEEPER.\n",
      "Citral_2: 50 stimulations with pure citral (10 s before / 1 s citral / 18 s after) 1 s between end and start.\n",
      "Citral_3: 50 stimulations with pure citral (10 s before / 1 s citral / 18 s after) 1 s between end and start.\n",
      "Continuous_3: 50 acquisitions 29 seconds long with 1 s between end and start.\n",
      "Continuous_4: 50 acquisitions 29 seconds long with 1 s between end and start. THE FIRST 45 ACQUISITIONS ARE AVAILABLE THE LAST 5 HAVE BEEN LOST (CD CORRUPTION).  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(hdf.attrs['LabBook'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "We can get the names of the different [groups](http://www.hdfgroup.org/HDF5/doc/UG/HDF5_Users_Guide-Responsive%20HTML5/index.html#t=HDF5_Users_Guide/Groups/HDF5_Groups.htm) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citral_1\n",
      "Citral_2\n",
      "Citral_3\n",
      "Continuous_1\n",
      "Continuous_2\n",
      "Continuous_3\n",
      "Continuous_4\n"
     ]
    }
   ],
   "source": [
    "for name in hdf:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "We can get the names of the first five subgroups of group `Continuous_1` with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trial_01', 'trial_02', 'trial_03', 'trial_04', 'trial_05']\n"
     ]
    }
   ],
   "source": [
    "Continuous_1_names = []\n",
    "for name in hdf['Continuous_1']:\n",
    "    Continuous_1_names.append(name)\n",
    "print(Continuous_1_names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "We see that our first command loaded in RAM the first 3 elements of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Continuous_1_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "trials.\n",
    "\n",
    "The content of the `log_file_content` attribute of group `Continuous_1`\n",
    "is visualized with (the first 518 characters are shown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Parameters:\n",
      "  number of trials: 90\n",
      "  trial length: 29 sec\n",
      "  delay to odor: 3 sec\n",
      "  odor duration: 1000 msec\n",
      "  interval between start of trials: 30 sec\n",
      "  master8 channel: 8\n",
      "Continue_1 started recording: \tThu Feb  1 16:26:11 2001\n",
      "Continue_1 stopped recording: \tThu Feb  1 16:26:40 2001\n",
      "Continue_1 started recording: \tThu Feb  1 16:26:41 2001\n",
      "Continue_1 stopped recording: \tThu Feb  1 16:27:10 2001\n",
      "Continue_1 started recording: \tThu Feb  1 16:27:11 2001\n",
      "Continue_1 stopped recording: \tThu Feb  1 16:27:40 2001\n"
     ]
    }
   ],
   "source": [
    "print(hdf['Continuous_1'].attrs['log_file_content'][:518])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "An import remark on the data\n",
    "----------------------------\n",
    "\n",
    "**The data came out of the A/D converter as 16 bit unsigned integers and were stored on the computer hard-drive as 32 bit unsigned integers** this explains the line `sigs = (sigs.astype('float32') - 2**15.) / 2**15` in the `for loop` of our first code block, where the 32 bit unsigned integers are converted to 32 bit floats with the proper offset to have zero at zero (this last maneuver is in fact superfluous since we will subsequently normalize the data). The data were moreover band-pass filtered between 300 and 5 kHz before A/D conversion and sampled at 15 kHz. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Loading modules and code\n",
    "========================\n",
    "\n",
    "We are going to use the usual scientific python modules, [Numpy](http://docs.scipy.org/doc/numpy/), [Matplotlib](http://matplotlib.org/contents.html), [Pandas](http://pandas.pydata.org/pandas-docs/stable/). We will also use [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/) and, of course, [tridesclous](https://github.com/tridesclous/tridesclous). So we start by loading all these modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "from tridesclous import SpikeSorter, SpikeSortingWindow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "name": "Analysis_example.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
